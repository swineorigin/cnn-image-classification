{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T14:24:23.462905Z",
     "iopub.status.busy": "2024-11-09T14:24:23.462458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "\n",
      "Training VGG-16...\n",
      "Epoch 1/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 940ms/step - accuracy: 0.3328 - loss: 1.9467 - val_accuracy: 0.4642 - val_loss: 1.6800 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 941ms/step - accuracy: 0.5220 - loss: 1.3352 - val_accuracy: 0.5318 - val_loss: 1.4139 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 937ms/step - accuracy: 0.6017 - loss: 1.1167 - val_accuracy: 0.5635 - val_loss: 1.4111 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 937ms/step - accuracy: 0.6492 - loss: 0.9933 - val_accuracy: 0.6795 - val_loss: 0.9087 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m736s\u001b[0m 941ms/step - accuracy: 0.6781 - loss: 0.9231 - val_accuracy: 0.6555 - val_loss: 1.0379 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 951ms/step - accuracy: 0.7016 - loss: 0.8557 - val_accuracy: 0.7210 - val_loss: 0.8634 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 945ms/step - accuracy: 0.7238 - loss: 0.7980 - val_accuracy: 0.5816 - val_loss: 1.4397 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 946ms/step - accuracy: 0.7400 - loss: 0.7590 - val_accuracy: 0.7423 - val_loss: 0.7480 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m730s\u001b[0m 934ms/step - accuracy: 0.7510 - loss: 0.7190 - val_accuracy: 0.7721 - val_loss: 0.6555 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m749s\u001b[0m 942ms/step - accuracy: 0.7654 - loss: 0.6829 - val_accuracy: 0.7813 - val_loss: 0.6430 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 939ms/step - accuracy: 0.7742 - loss: 0.6486 - val_accuracy: 0.7625 - val_loss: 0.7087 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m731s\u001b[0m 934ms/step - accuracy: 0.7881 - loss: 0.6202 - val_accuracy: 0.7575 - val_loss: 0.7427 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m738s\u001b[0m 944ms/step - accuracy: 0.8125 - loss: 0.5433 - val_accuracy: 0.8273 - val_loss: 0.5138 - learning_rate: 2.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 940ms/step - accuracy: 0.8283 - loss: 0.4969 - val_accuracy: 0.8370 - val_loss: 0.4942 - learning_rate: 2.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 930ms/step - accuracy: 0.8342 - loss: 0.4818 - val_accuracy: 0.8405 - val_loss: 0.4912 - learning_rate: 2.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m733s\u001b[0m 938ms/step - accuracy: 0.8391 - loss: 0.4644 - val_accuracy: 0.8337 - val_loss: 0.5072 - learning_rate: 2.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m747s\u001b[0m 944ms/step - accuracy: 0.8410 - loss: 0.4574 - val_accuracy: 0.8487 - val_loss: 0.4548 - learning_rate: 2.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m776/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 904ms/step - accuracy: 0.8412 - loss: 0.4563"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Define class names\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Enhanced data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomTranslation(0.1, 0.1)\n",
    "])\n",
    "\n",
    "def create_vgg16():\n",
    "    model = models.Sequential([\n",
    "        data_augmentation,\n",
    "        layers.Conv2D(64, (3, 3), padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_vgg19():\n",
    "    model = models.Sequential([\n",
    "        data_augmentation,\n",
    "        layers.Conv2D(64, (3, 3), padding='same', input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(64, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(128, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, (3, 3), padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_resnet18():\n",
    "    def basic_block(x, filters, stride=1, downsample=None):\n",
    "        identity = x\n",
    "        \n",
    "        out = layers.Conv2D(filters, kernel_size=3, strides=stride, padding='same')(x)\n",
    "        out = layers.BatchNormalization()(out)\n",
    "        out = layers.Activation('relu')(out)\n",
    "        \n",
    "        out = layers.Conv2D(filters, kernel_size=3, padding='same')(out)\n",
    "        out = layers.BatchNormalization()(out)\n",
    "        \n",
    "        if downsample is not None:\n",
    "            identity = downsample(x)\n",
    "        \n",
    "        out = layers.Add()([out, identity])\n",
    "        out = layers.Activation('relu')(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    # layer1\n",
    "    for _ in range(2):\n",
    "        x = basic_block(x, 64)\n",
    "    \n",
    "    # layer2\n",
    "    downsample = lambda x: layers.Conv2D(128, kernel_size=1, strides=2)(x)\n",
    "    x = basic_block(x, 128, stride=2, downsample=downsample)\n",
    "    x = basic_block(x, 128)\n",
    "    \n",
    "    # layer3\n",
    "    downsample = lambda x: layers.Conv2D(256, kernel_size=1, strides=2)(x)\n",
    "    x = basic_block(x, 256, stride=2, downsample=downsample)\n",
    "    x = basic_block(x, 256)\n",
    "    \n",
    "    # layer4\n",
    "    downsample = lambda x: layers.Conv2D(512, kernel_size=1, strides=2)(x)\n",
    "    x = basic_block(x, 512, stride=2, downsample=downsample)\n",
    "    x = basic_block(x, 512)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs, x)\n",
    "\n",
    "def create_resnet34():\n",
    "    def basic_block(x, filters, stride=1, downsample=None):\n",
    "        identity = x\n",
    "        \n",
    "        out = layers.Conv2D(filters, kernel_size=3, strides=stride, padding='same')(x)\n",
    "        out = layers.BatchNormalization()(out)\n",
    "        out = layers.Activation('relu')(out)\n",
    "        \n",
    "        out = layers.Conv2D(filters, kernel_size=3, padding='same')(out)\n",
    "        out = layers.BatchNormalization()(out)\n",
    "        \n",
    "        if downsample is not None:\n",
    "            identity = downsample(x)\n",
    "        \n",
    "        out = layers.Add()([out, identity])\n",
    "        out = layers.Activation('relu')(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    inputs = layers.Input(shape=(32, 32, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    \n",
    "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=3, strides=2, padding='same')(x)\n",
    "    \n",
    "    # layer1\n",
    "    for _ in range(3):\n",
    "        x = basic_block(x, 64)\n",
    "    \n",
    "    # layer2\n",
    "    downsample = lambda x: layers.Conv2D(128, kernel_size=1, strides=2)(x)\n",
    "    x = basic_block(x, 128, stride=2, downsample=downsample)\n",
    "    for _ in range(3):\n",
    "        x = basic_block(x, 128)\n",
    "    \n",
    "    # layer3\n",
    "    downsample = lambda x: layers.Conv2D(256, kernel_size=1, strides=2)(x)\n",
    "    x = basic_block(x, 256, stride=2, downsample=downsample)\n",
    "    for _ in range(5):\n",
    "        x = basic_block(x, 256)\n",
    "    \n",
    "    # layer4\n",
    "    downsample = lambda x: layers.Conv2D(512, kernel_size=1, strides=2)(x)\n",
    "    x = basic_block(x, 512, stride=2, downsample=downsample)\n",
    "    for _ in range(2):\n",
    "        x = basic_block(x, 512)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs, x)\n",
    "\n",
    "def create_resnet50():\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(32, 32, 3)\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    \n",
    "    model = models.Sequential([\n",
    "        data_augmentation,\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(256),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create models\n",
    "models_dict = {\n",
    "    'VGG-16': create_vgg16(),\n",
    "    'VGG-19': create_vgg19(),\n",
    "    'ResNet-18': create_resnet18(),\n",
    "    'ResNet-34': create_resnet34(),\n",
    "    'ResNet-50': create_resnet50()\n",
    "}\n",
    "\n",
    "# Compile models\n",
    "def compile_model(model):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)  # Fixed learning rate\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "for model in models_dict.values():\n",
    "    compile_model(model)\n",
    "\n",
    "# Callbacks for training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=2,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train models\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "\n",
    "histories = {}\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    histories[name] = model.fit(\n",
    "        train_images, train_labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(test_images, test_labels),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model):\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "    train_loss, train_acc = model.evaluate(train_images, train_labels, verbose=0)\n",
    "    \n",
    "    return {\n",
    "        'Training Accuracy': f'{train_acc:.2%}',\n",
    "        'Test Accuracy': f'{test_acc:.2%}',\n",
    "        'Training Loss': f'{train_loss:.4f}',\n",
    "        'Test Loss': f'{test_loss:.4f}'\n",
    "    }\n",
    "\n",
    "# Create performance comparison table\n",
    "model_performance = {\n",
    "    name: evaluate_model(model) for name, model in models_dict.items()\n",
    "}\n",
    "\n",
    "# Print performance table\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Architecture':<12} {'Training Accuracy':<18} {'Test Accuracy':<15} {'Training Loss':<15} {'Test Loss':<15}\")\n",
    "print(\"-\" * 80)\n",
    "for model_name, metrics in model_performance.items():\n",
    "    print(f\"{model_name:<12} {metrics['Training Accuracy']:<18} {metrics['Test Accuracy']:<15} {metrics['Training Loss']:<15} {metrics['Test Loss']:<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Plot training histories\n",
    "def plot_all_histories():\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, history in histories.items():\n",
    "        plt.plot(history.history['accuracy'], label=f'{name} (Train)')\n",
    "        plt.plot(history.history['val_accuracy'], label=f'{name} (Val)', linestyle='--')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, history in histories.items():\n",
    "        plt.plot(history.history['loss'], label=f'{name} (Train)')\n",
    "        plt.plot(history.history['val_loss'], label=f'{name} (Val)', linestyle='--')\n",
    "    plt.title('Model Loss Comparison')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_all_histories()\n",
    "\n",
    "# Save models\n",
    "for name, model in models_dict.items():\n",
    "    model.save(f'{name.lower()}_cifar10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T22:09:41.091803Z",
     "iopub.status.busy": "2024-11-09T22:09:41.091165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 290/1563\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:25\u001b[0m 397ms/step - accuracy: 0.1012 - loss: 2.3117"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Define class names for CIFAR-10 dataset\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit data generator on training data\n",
    "datagen.fit(train_images)\n",
    "\n",
    "# Define Models\n",
    "def create_vgg16():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_vgg19():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_resnet18():\n",
    "    base_model = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3))\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_resnet34():\n",
    "    base_model = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3))\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_resnet50():\n",
    "    base_model = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=(32, 32, 3))\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create models\n",
    "vgg16_model = create_vgg16()\n",
    "vgg19_model = create_vgg19()\n",
    "resnet18_model = create_resnet18()\n",
    "resnet34_model = create_resnet34()\n",
    "resnet50_model = create_resnet50()\n",
    "\n",
    "# Compile Models\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "compile_model(vgg16_model)\n",
    "compile_model(vgg19_model)\n",
    "compile_model(resnet18_model)\n",
    "compile_model(resnet34_model)\n",
    "compile_model(resnet50_model)\n",
    "\n",
    "# Train Models\n",
    "epochs = 10\n",
    "\n",
    "vgg16_history = vgg16_model.fit(datagen.flow(train_images, train_labels, batch_size=32), epochs=epochs, validation_data=(test_images, test_labels))\n",
    "vgg19_history = vgg19_model.fit(datagen.flow(train_images, train_labels, batch_size=32), epochs=epochs, validation_data=(test_images, test_labels))\n",
    "resnet18_history = resnet18_model.fit(datagen.flow(train_images, train_labels, batch_size=32), epochs=epochs, validation_data=(test_images, test_labels))\n",
    "resnet34_history = resnet34_model.fit(datagen.flow(train_images, train_labels, batch_size=32), epochs=epochs, validation_data=(test_images, test_labels))\n",
    "resnet50_history = resnet50_model.fit(datagen.flow(train_images, train_labels, batch_size=32), epochs=epochs, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate Models\n",
    "vgg16_test_loss, vgg16_test_acc = vgg16_model.evaluate(test_images, test_labels)\n",
    "vgg19_test_loss, vgg19_test_acc = vgg19_model.evaluate(test_images, test_labels)\n",
    "resnet18_test_loss, resnet18_test_acc = resnet18_model.evaluate(test_images, test_labels)\n",
    "resnet34_test_loss, resnet34_test_acc = resnet34_model.evaluate(test_images, test_labels)\n",
    "resnet50_test_loss, resnet50_test_acc = resnet50_model.evaluate(test_images, test_labels)\n",
    "\n",
    "print(f'VGG16 Test accuracy: {vgg16_test_acc}')\n",
    "print(f'VGG19 Test accuracy: {vgg19_test_acc}')\n",
    "print(f'ResNet18 Test accuracy: {resnet18_test_acc}')\n",
    "print(f'ResNet34 Test accuracy: {resnet34_test_acc}')\n",
    "print(f'ResNet50 Test accuracy: {resnet50_test_acc}')\n",
    "\n",
    "# Plot Training History\n",
    "def plot_history(history, title):\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(f'{title} Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "plot_history(vgg16_history, 'VGG16')\n",
    "plot_history(vgg19_history, 'VGG19')\n",
    "plot_history(resnet18_history, 'ResNet18')\n",
    "plot_history(resnet34_history, 'ResNet34')\n",
    "plot_history(resnet50_history, 'ResNet50')\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(model, test_images, test_labels, title):\n",
    "    y_pred = model.predict(test_images)\n",
    "    y_pred_classes = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf_matrix = confusion_matrix(test_labels.flatten(), y_pred_classes)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
